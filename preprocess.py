# https://github.com/nguyenvanhieuvn/text-classification-tutorial/blob/master/text_classification_tutorial.ipynb
# https://nguyenvanhieu.vn/phan-loai-van-ban-tieng-viet

import regex as re
import string
import emoji

from vncorenlp import VnCoreNLP
from nltk import flatten


# Remove HTML code
def remove_HTML(text):
    return re.sub(r'<[^>]*>', '', text)


# Standardize unicode
def convert_unicode(text):
    char1252 = 'aÌ€|aÌ|aÌ‰|aÌƒ|aÌ£|Ã¢Ì€|Ã¢Ì|Ã¢Ì‰|Ã¢Ìƒ|Ã¢Ì£|ÄƒÌ€|ÄƒÌ|ÄƒÌ‰|ÄƒÌƒ|ÄƒÌ£|eÌ€|eÌ|eÌ‰|eÌƒ|eÌ£|ÃªÌ€|ÃªÌ|ÃªÌ‰|ÃªÌƒ|ÃªÌ£|iÌ€|iÌ|iÌ‰|iÌƒ|iÌ£|oÌ€|oÌ|oÌ‰|oÌƒ|oÌ£|Ã´Ì€|Ã´Ì|Ã´Ì‰|Ã´Ìƒ|Ã´Ì£|Æ¡Ì€|Æ¡Ì|Æ¡Ì‰|Æ¡Ìƒ|Æ¡Ì£|uÌ€|uÌ|uÌ‰|uÌƒ|uÌ£|Æ°Ì€|Æ°Ì|Æ°Ì‰|Æ°Ìƒ|Æ°Ì£|yÌ€|yÌ|yÌ‰|yÌƒ|yÌ£|AÌ€|AÌ|AÌ‰|AÌƒ|AÌ£|Ã‚Ì€|Ã‚Ì|Ã‚Ì‰|Ã‚Ìƒ|Ã‚Ì£|Ä‚Ì€|Ä‚Ì|Ä‚Ì‰|Ä‚Ìƒ|Ä‚Ì£|EÌ€|EÌ|EÌ‰|EÌƒ|EÌ£|ÃŠÌ€|ÃŠÌ|ÃŠÌ‰|ÃŠÌƒ|ÃŠÌ£|IÌ€|IÌ|IÌ‰|IÌƒ|IÌ£|OÌ€|OÌ|OÌ‰|OÌƒ|OÌ£|Ã”Ì€|Ã”Ì|Ã”Ì‰|Ã”Ìƒ|Ã”Ì£|Æ Ì€|Æ Ì|Æ Ì‰|Æ Ìƒ|Æ Ì£|UÌ€|UÌ|UÌ‰|UÌƒ|UÌ£|Æ¯Ì€|Æ¯Ì|Æ¯Ì‰|Æ¯Ìƒ|Æ¯Ì£|YÌ€|YÌ|YÌ‰|YÌƒ|YÌ£'
    charutf8 = 'Ã |Ã¡|áº£|Ã£|áº¡|áº§|áº¥|áº©|áº«|áº­|áº±|áº¯|áº³|áºµ|áº·|Ã¨|Ã©|áº»|áº½|áº¹|á»|áº¿|á»ƒ|á»…|á»‡|Ã¬|Ã­|á»‰|Ä©|á»‹|Ã²|Ã³|á»|Ãµ|á»|á»“|á»‘|á»•|á»—|á»™|á»|á»›|á»Ÿ|á»¡|á»£|Ã¹|Ãº|á»§|Å©|á»¥|á»«|á»©|á»­|á»¯|á»±|á»³|Ã½|á»·|á»¹|á»µ|Ã€|Ã|áº¢|Ãƒ|áº |áº¦|áº¤|áº¨|áºª|áº¬|áº°|áº®|áº²|áº´|áº¶|Ãˆ|Ã‰|áºº|áº¼|áº¸|á»€|áº¾|á»‚|á»„|á»†|ÃŒ|Ã|á»ˆ|Ä¨|á»Š|Ã’|Ã“|á»|Ã•|á»Œ|á»’|á»|á»”|á»–|á»˜|á»œ|á»š|á»|á» |á»¢|Ã™|Ãš|á»¦|Å¨|á»¤|á»ª|á»¨|á»¬|á»®|á»°|á»²|Ã|á»¶|á»¸|á»´'
    char1252 = char1252.split('|')
    charutf8 = charutf8.split('|')
    
    dic = {}
    for i in range(len(char1252)): dic[char1252[i]] = charutf8[i]
    return re.sub(
        r'aÌ€|aÌ|aÌ‰|aÌƒ|aÌ£|Ã¢Ì€|Ã¢Ì|Ã¢Ì‰|Ã¢Ìƒ|Ã¢Ì£|ÄƒÌ€|ÄƒÌ|ÄƒÌ‰|ÄƒÌƒ|ÄƒÌ£|eÌ€|eÌ|eÌ‰|eÌƒ|eÌ£|ÃªÌ€|ÃªÌ|ÃªÌ‰|ÃªÌƒ|ÃªÌ£|iÌ€|iÌ|iÌ‰|iÌƒ|iÌ£|oÌ€|oÌ|oÌ‰|oÌƒ|oÌ£|Ã´Ì€|Ã´Ì|Ã´Ì‰|Ã´Ìƒ|Ã´Ì£|Æ¡Ì€|Æ¡Ì|Æ¡Ì‰|Æ¡Ìƒ|Æ¡Ì£|uÌ€|uÌ|uÌ‰|uÌƒ|uÌ£|Æ°Ì€|Æ°Ì|Æ°Ì‰|Æ°Ìƒ|Æ°Ì£|yÌ€|yÌ|yÌ‰|yÌƒ|yÌ£|AÌ€|AÌ|AÌ‰|AÌƒ|AÌ£|Ã‚Ì€|Ã‚Ì|Ã‚Ì‰|Ã‚Ìƒ|Ã‚Ì£|Ä‚Ì€|Ä‚Ì|Ä‚Ì‰|Ä‚Ìƒ|Ä‚Ì£|EÌ€|EÌ|EÌ‰|EÌƒ|EÌ£|ÃŠÌ€|ÃŠÌ|ÃŠÌ‰|ÃŠÌƒ|ÃŠÌ£|IÌ€|IÌ|IÌ‰|IÌƒ|IÌ£|OÌ€|OÌ|OÌ‰|OÌƒ|OÌ£|Ã”Ì€|Ã”Ì|Ã”Ì‰|Ã”Ìƒ|Ã”Ì£|Æ Ì€|Æ Ì|Æ Ì‰|Æ Ìƒ|Æ Ì£|UÌ€|UÌ|UÌ‰|UÌƒ|UÌ£|Æ¯Ì€|Æ¯Ì|Æ¯Ì‰|Æ¯Ìƒ|Æ¯Ì£|YÌ€|YÌ|YÌ‰|YÌƒ|YÌ£',
        lambda x: dic[x.group()], text
    )


# Standardize accent typing
vowels_to_ids = {}
vowels_table = [
    ['a', 'Ã ', 'Ã¡', 'áº£', 'Ã£', 'áº¡', 'a' ],
    ['Äƒ', 'áº±', 'áº¯', 'áº³', 'áºµ', 'áº·', 'aw'],
    ['Ã¢', 'áº§', 'áº¥', 'áº©', 'áº«', 'áº­', 'aa'],
    ['e', 'Ã¨', 'Ã©', 'áº»', 'áº½', 'áº¹', 'e' ],
    ['Ãª', 'á»', 'áº¿', 'á»ƒ', 'á»…', 'á»‡', 'ee'],
    ['i', 'Ã¬', 'Ã­', 'á»‰', 'Ä©', 'á»‹', 'i' ],
    ['o', 'Ã²', 'Ã³', 'á»', 'Ãµ', 'á»', 'o' ],
    ['Ã´', 'á»“', 'á»‘', 'á»•', 'á»—', 'á»™', 'oo'],
    ['Æ¡', 'á»', 'á»›', 'á»Ÿ', 'á»¡', 'á»£', 'ow'],
    ['u', 'Ã¹', 'Ãº', 'á»§', 'Å©', 'á»¥', 'u' ],
    ['Æ°', 'á»«', 'á»©', 'á»­', 'á»¯', 'á»±', 'uw'],
    ['y', 'á»³', 'Ã½', 'á»·', 'á»¹', 'á»µ', 'y' ]
]

for i in range(len(vowels_table)):
    for j in range(len(vowels_table[i]) - 1):
        vowels_to_ids[vowels_table[i][j]] = (i, j)


def is_valid_vietnamese_word(word):
    chars = list(word)
    vowel_indexes = -1
    for index, char in enumerate(chars):
        x, y = vowels_to_ids.get(char, (-1, -1))
        if x != -1:
            if vowel_indexes == -1: vowel_indexes = index
            else:
                if index - vowel_indexes != 1: return False
                vowel_indexes = index
    return True


def standardize_word_typing(word):
    if not is_valid_vietnamese_word(word): return word
    chars = list(word)
    dau_cau = 0
    vowel_indexes = []
    qu_or_gi = False

    for index, char in enumerate(chars):
        x, y = vowels_to_ids.get(char, (-1, -1))
        if x == -1: continue
        elif x == 9:  # check qu
            if index != 0 and chars[index - 1] == 'q':
                chars[index] = 'u'
                qu_or_gi = True
        elif x == 5:  # check gi
            if index != 0 and chars[index - 1] == 'g':
                chars[index] = 'i'
                qu_or_gi = True

        if y != 0:
            dau_cau = y
            chars[index] = vowels_table[x][0]

        if not qu_or_gi or index != 1:
            vowel_indexes.append(index)

    if len(vowel_indexes) < 2:
        if qu_or_gi:
            if len(chars) == 2:
                x, y = vowels_to_ids.get(chars[1])
                chars[1] = vowels_table[x][dau_cau]
            else:
                x, y = vowels_to_ids.get(chars[2], (-1, -1))
                if x != -1: chars[2] = vowels_table[x][dau_cau]
                else: chars[1] = vowels_table[5][dau_cau] if chars[1] == 'i' else vowels_table[9][dau_cau]
            return ''.join(chars)
        return word

    for index in vowel_indexes:
        x, y = vowels_to_ids[chars[index]]
        if x == 4 or x == 8:  # Ãª, Æ¡
            chars[index] = vowels_table[x][dau_cau]
            return ''.join(chars)

    if len(vowel_indexes) == 2:
        if vowel_indexes[-1] == len(chars) - 1:
            x, y = vowels_to_ids[chars[vowel_indexes[0]]]
            chars[vowel_indexes[0]] = vowels_table[x][dau_cau]
        else:
            x, y = vowels_to_ids[chars[vowel_indexes[1]]]
            chars[vowel_indexes[1]] = vowels_table[x][dau_cau]
    else:
        x, y = vowels_to_ids[chars[vowel_indexes[1]]]
        chars[vowel_indexes[1]] = vowels_table[x][dau_cau]
    return ''.join(chars)


def standardize_sentence_typing(text):
    words = text.lower().split()
    for index, word in enumerate(words):
        cw = re.sub(r'(^\p{P}*)([p{L}.]*\p{L}+)(\p{P}*$)', r'\1/\2/\3', word).split('/')
        if len(cw) == 3: cw[1] = standardize_word_typing(cw[1])
        words[index] = ''.join(cw)
    return ' '.join(words)


# Normalize acronyms
# !wget https://gist.githubusercontent.com/nguyenvanhieuvn/7d9441c10b3c2739499fc5a4d9ea06fb/raw/df939245b3e841b62af115be4dcb3516dadc9fc5/teencode.txt
replace_list = {
    'Ã´ kÃªi': 'ok', 'okie': 'ok', 'o kÃª': 'ok', 'okey': 'ok', 'Ã´kÃª': 'ok', 'oki': 'ok', 'oke': 'ok', 'okay': 'ok', 'okÃª': 'ok',
    'tks': 'cáº£m Æ¡n', 'thks': 'cáº£m Æ¡n', 'thanks': 'cáº£m Æ¡n', 'ths': 'cáº£m Æ¡n', 'thank': 'cáº£m Æ¡n',
    'kg': 'khÃ´ng', 'not': 'khÃ´ng', 'k': 'khÃ´ng', 'kh': 'khÃ´ng', 'kÃ´': 'khÃ´ng', 'hok': 'khÃ´ng', 'ko': 'khÃ´ng', 'khong': 'khÃ´ng', 'kp': 'khÃ´ng pháº£i',
    'he he': 'tÃ­ch cá»±c', 'hehe': 'tÃ­ch cá»±c', 'hihi': 'tÃ­ch cá»±c', 'haha': 'tÃ­ch cá»±c', 'hjhj': 'tÃ­ch cá»±c', 'thick': 'tÃ­ch cá»±c',
    'lol': 'tiÃªu cá»±c', 'cc': 'tiÃªu cá»±c', 'huhu': 'tiÃªu cá»±c', 'cute': 'dá»… thÆ°Æ¡ng',
     
    'sz': 'cá»¡', 'size': 'cá»¡', 
    'wa': 'quÃ¡', 'wÃ¡': 'quÃ¡', 'qÃ¡': 'quÃ¡', 
    'Ä‘x': 'Ä‘Æ°á»£c', 'dk': 'Ä‘Æ°á»£c', 'dc': 'Ä‘Æ°á»£c', 'Ä‘k': 'Ä‘Æ°á»£c', 'Ä‘c': 'Ä‘Æ°á»£c', 
    'vs': 'vá»›i', 'j': 'gÃ¬', 'â€œ': ' ', 'time': 'thá»i gian', 'm': 'mÃ¬nh', 'mik': 'mÃ¬nh', 'r': 'rá»“i', 'bjo': 'bao giá»', 'very': 'ráº¥t',

    'authentic': 'chuáº©n chÃ­nh hÃ£ng', 'aut': 'chuáº©n chÃ­nh hÃ£ng', 'auth': 'chuáº©n chÃ­nh hÃ£ng', 'date': 'háº¡n sá»­ dá»¥ng', 'hsd': 'háº¡n sá»­ dá»¥ng', 
    'store': 'cá»­a hÃ ng', 'sop': 'cá»­a hÃ ng', 'shopE': 'cá»­a hÃ ng', 'shop': 'cá»­a hÃ ng', 
    'sp': 'sáº£n pháº©m', 'product': 'sáº£n pháº©m', 'hÃ g': 'hÃ ng', 
    'ship': 'giao hÃ ng', 'delivery': 'giao hÃ ng', 'sÃ­p': 'giao hÃ ng', 'order': 'Ä‘áº·t hÃ ng',

    'gud': 'tá»‘t', 'wel done': 'tá»‘t', 'good': 'tá»‘t', 'gÃºt': 'tá»‘t', 'tot': 'tá»‘t', 'nice': 'tá»‘t', 'perfect': 'ráº¥t tá»‘t', 
    'quality': 'cháº¥t lÆ°á»£ng', 'cháº¥t lg': 'cháº¥t lÆ°á»£ng', 'chat': 'cháº¥t', 'excelent': 'hoÃ n háº£o', 'bt': 'bÃ¬nh thÆ°á»ng',
    'sad': 'tá»‡', 'por': 'tá»‡', 'poor': 'tá»‡', 'bad': 'tá»‡', 
    'beautiful': 'Ä‘áº¹p tuyá»‡t vá»i', 'dep': 'Ä‘áº¹p', 
    'xau': 'xáº¥u', 'sáº¥u': 'xáº¥u', 
     
    'thik': 'thÃ­ch', 'iu': 'yÃªu', 'fake': 'giáº£ máº¡o', 
    'quickly': 'nhanh', 'quick': 'nhanh', 'fast': 'nhanh',
    'fresh': 'tÆ°Æ¡i', 'delicious': 'ngon',

    'dt': 'Ä‘iá»‡n thoáº¡i', 'fb': 'facebook', 'face': 'facebook', 'ks': 'khÃ¡ch sáº¡n', 'nv': 'nhÃ¢n viÃªn',
    'nt': 'nháº¯n tin', 'ib': 'nháº¯n tin', 'tl': 'tráº£ lá»i', 'trl': 'tráº£ lá»i', 'rep': 'tráº£ lá»i',
    'fback': 'feedback', 'fedback': 'feedback',
    'sd': 'sá»­ dá»¥ng', 'sÃ i': 'xÃ i', 

    '^_^': 'tÃ­ch cá»±c', ':)': 'tÃ­ch cá»±c', ':(': 'tiÃªu cá»±c',
    'â¤ï¸': 'tÃ­ch cá»±c', 'ğŸ‘': 'tÃ­ch cá»±c', 'ğŸ‰': 'tÃ­ch cá»±c', 'ğŸ˜€': 'tÃ­ch cá»±c', 'ğŸ˜': 'tÃ­ch cá»±c', 'ğŸ˜‚': 'tÃ­ch cá»±c', 'ğŸ¤—': 'tÃ­ch cá»±c', 'ğŸ˜™': 'tÃ­ch cá»±c', 'ğŸ™‚': 'tÃ­ch cá»±c', 
    'ğŸ˜”': 'tiÃªu cá»±c', 'ğŸ˜“': 'tiÃªu cá»±c', 
    'â­': 'star', '*': 'star', 'ğŸŒŸ': 'star',
}

with open('teencode.txt', encoding='utf-8') as f:
    for pair in f.readlines():
        key, value = pair.split('\t')
        replace_list[key] = value.strip()


def normalize_acronyms(text):
    words = []
    for word in text.strip().split():
        # word = word.strip(string.punctuation)
        if word.lower() not in replace_list.keys(): words.append(word)
        else: words.append(replace_list[word.lower()])
    return emoji.demojize(' '.join(words)) # Remove Emojis


# Word segmentation
annotator = VnCoreNLP('VnCoreNLP/VnCoreNLP-1.1.1.jar') 
def word_segmentation(text):
    words = annotator.tokenize(text)
    return ' '.join(word for word in flatten(words))


# Remove unnecessary characters
def remove_unnecessary_characters(text):
    text = re.sub(r'[^\s\wÃ¡Ã áº£Ã£áº¡Äƒáº¯áº±áº³áºµáº·Ã¢áº¥áº§áº©áº«áº­Ã©Ã¨áº»áº½áº¹Ãªáº¿á»á»ƒá»…á»‡Ã³Ã²á»Ãµá»Ã´á»‘á»“á»•á»—á»™Æ¡á»›á»á»Ÿá»¡á»£Ã­Ã¬á»‰Ä©á»‹ÃºÃ¹á»§Å©á»¥Æ°á»©á»«á»­á»¯á»±Ã½á»³á»·á»¹á»µÄ‘ÃÃ€áº¢Ãƒáº Ä‚áº®áº°áº²áº´áº¶Ã‚áº¤áº¦áº¨áºªáº¬Ã‰Ãˆáººáº¼áº¸ÃŠáº¾á»€á»‚á»„á»†Ã“Ã’á»Ã•á»ŒÃ”á»á»’á»”á»–á»˜Æ á»šá»œá»á» á»¢ÃÃŒá»ˆÄ¨á»ŠÃšÃ™á»¦Å¨á»¤Æ¯á»¨á»ªá»¬á»®á»°Ãá»²á»¶á»¸á»´Ä_]', ' ', text)
    text = re.sub(r'\s+', ' ', text).strip() # Remove extra whitespace
    return text