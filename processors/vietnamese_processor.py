# https://github.com/behitek/text-classification-tutorial/blob/master/text_classification_tutorial.ipynb
import os
import emoji
import urllib
import requests
import regex as re

from io import StringIO
from vncorenlp import VnCoreNLP
from transformers import pipeline


class VietnameseTextCleaner: # https://ihateregex.io
    VN_CHARS = '√°√†·∫£√£·∫°ƒÉ·∫Ø·∫±·∫≥·∫µ·∫∑√¢·∫•·∫ß·∫©·∫´·∫≠√©√®·∫ª·∫Ω·∫π√™·∫ø·ªÅ·ªÉ·ªÖ·ªá√≥√≤·ªè√µ·ªç√¥·ªë·ªì·ªï·ªó·ªô∆°·ªõ·ªù·ªü·ª°·ª£√≠√¨·ªâƒ©·ªã√∫√π·ªß≈©·ª•∆∞·ª©·ª´·ª≠·ªØ·ª±√Ω·ª≥·ª∑·ªπ·ªµƒë√Å√Ä·∫¢√É·∫†ƒÇ·∫Æ·∫∞·∫≤·∫¥·∫∂√Ç·∫§·∫¶·∫®·∫™·∫¨√â√à·∫∫·∫º·∫∏√ä·∫æ·ªÄ·ªÇ·ªÑ·ªÜ√ì√í·ªé√ï·ªå√î·ªê·ªí·ªî·ªñ·ªò∆†·ªö·ªú·ªû·ª†·ª¢√ç√å·ªàƒ®·ªä√ö√ô·ª¶≈®·ª§∆Ø·ª®·ª™·ª¨·ªÆ·ª∞√ù·ª≤·ª∂·ª∏·ª¥ƒê'
    
    @staticmethod
    def remove_html(text):
        return re.sub(r'<[^>]*>', '', text)
    
    @staticmethod
    def remove_emoji(text):
        return emoji.replace_emoji(text, '')
    
    @staticmethod
    def remove_url(text):
        return re.sub(r'https?:\/\/(www\.)?[-a-zA-Z0-9@:%._\+~#=]{1,256}\.[a-zA-Z0-9()]{1,6}\b([-a-zA-Z0-9()!@:%_\+.~#?&\/\/=]*)', '', text)
    
    @staticmethod
    def remove_email(text):
        return re.sub(r'[^@ \t\r\n]+@[^@ \t\r\n]+\.[^@ \t\r\n]+', '', text)
    
    @staticmethod
    def remove_phone_number(text):
        return re.sub(r'^[\+]?[(]?[0-9]{3}[)]?[-\s\.]?[0-9]{3}[-\s\.]?[0-9]{4,6}$', '', text)
    
    @staticmethod
    def remove_hashtags(text):
        return re.sub(r'#\w+', '', text)
    
    @staticmethod
    def remove_unnecessary_characters(text):
        text = re.sub(fr"[^\sa-zA-Z0-9{VietnameseTextCleaner.VN_CHARS}]", ' ', text)
        return re.sub(r'\s+', ' ', text).strip() # Remove extra whitespace
    
    @staticmethod
    def process_text(text):
        text = VietnameseTextCleaner.remove_html(text)
        text = VietnameseTextCleaner.remove_emoji(text)
        text = VietnameseTextCleaner.remove_url(text)
        text = VietnameseTextCleaner.remove_email(text)
        text = VietnameseTextCleaner.remove_phone_number(text)
        text = VietnameseTextCleaner.remove_hashtags(text)
        return VietnameseTextCleaner.remove_unnecessary_characters(text)


class VietnameseToneNormalizer:
    VOWELS_TABLE = [
        ['a', '√†', '√°', '·∫£', '√£', '·∫°', 'a'],
        ['ƒÉ', '·∫±', '·∫Ø', '·∫≥', '·∫µ', '·∫∑', 'aw'],
        ['√¢', '·∫ß', '·∫•', '·∫©', '·∫´', '·∫≠', 'aa'],
        ['e', '√®', '√©', '·∫ª', '·∫Ω', '·∫π', 'e' ],
        ['√™', '·ªÅ', '·∫ø', '·ªÉ', '·ªÖ', '·ªá', 'ee'],
        ['i', '√¨', '√≠', '·ªâ', 'ƒ©', '·ªã', 'i' ],
        ['o', '√≤', '√≥', '·ªè', '√µ', '·ªç', 'o' ],
        ['√¥', '·ªì', '·ªë', '·ªï', '·ªó', '·ªô', 'oo'],
        ['∆°', '·ªù', '·ªõ', '·ªü', '·ª°', '·ª£', 'ow'],
        ['u', '√π', '√∫', '·ªß', '≈©', '·ª•', 'u' ],
        ['∆∞', '·ª´', '·ª©', '·ª≠', '·ªØ', '·ª±', 'uw'],
        ['y', '·ª≥', '√Ω', '·ª∑', '·ªπ', '·ªµ', 'y']
    ]
    
    # VOWELS_TO_IDS = {}
    # for i, row in enumerate(VOWELS_TABLE):
    #     for j, char in enumerate(row[:-1]):
    #         VOWELS_TO_IDS[char] = (i, j)
    VOWELS_TO_IDS = {
        'a': (0, 0), '√†': (0, 1), '√°': (0, 2), '·∫£': (0, 3), '√£': (0, 4), '·∫°': (0, 5), 
        'ƒÉ': (1, 0), '·∫±': (1, 1), '·∫Ø': (1, 2), '·∫≥': (1, 3), '·∫µ': (1, 4), '·∫∑': (1, 5), 
        '√¢': (2, 0), '·∫ß': (2, 1), '·∫•': (2, 2), '·∫©': (2, 3), '·∫´': (2, 4), '·∫≠': (2, 5), 
        'e': (3, 0), '√®': (3, 1), '√©': (3, 2), '·∫ª': (3, 3), '·∫Ω': (3, 4), '·∫π': (3, 5), 
        '√™': (4, 0), '·ªÅ': (4, 1), '·∫ø': (4, 2), '·ªÉ': (4, 3), '·ªÖ': (4, 4), '·ªá': (4, 5), 
        'i': (5, 0), '√¨': (5, 1), '√≠': (5, 2), '·ªâ': (5, 3), 'ƒ©': (5, 4), '·ªã': (5, 5), 
        'o': (6, 0), '√≤': (6, 1), '√≥': (6, 2), '·ªè': (6, 3), '√µ': (6, 4), '·ªç': (6, 5), 
        '√¥': (7, 0), '·ªì': (7, 1), '·ªë': (7, 2), '·ªï': (7, 3), '·ªó': (7, 4), '·ªô': (7, 5), 
        '∆°': (8, 0), '·ªù': (8, 1), '·ªõ': (8, 2), '·ªü': (8, 3), '·ª°': (8, 4), '·ª£': (8, 5), 
        'u': (9, 0), '√π': (9, 1), '√∫': (9, 2), '·ªß': (9, 3), '≈©': (9, 4), '·ª•': (9, 5), 
        '∆∞': (10, 0), '·ª´': (10, 1), '·ª©': (10, 2), '·ª≠': (10, 3), '·ªØ': (10, 4), '·ª±': (10, 5), 
        'y': (11, 0), '·ª≥': (11, 1), '√Ω': (11, 2), '·ª∑': (11, 3), '·ªπ': (11, 4), '·ªµ': (11, 5)
    }
    
    VINAI_NORMALIZED_TONE = {
        '√≤a': 'o√†', '√ía': 'O√†', '√íA': 'O√Ä', 
        '√≥a': 'o√°', '√ìa': 'O√°', '√ìA': 'O√Å', 
        '·ªèa': 'o·∫£', '·ªéa': 'O·∫£', '·ªéA': 'O·∫¢',
        '√µa': 'o√£', '√ïa': 'O√£', '√ïA': 'O√É',
        '·ªça': 'o·∫°', '·ªåa': 'O·∫°', '·ªåA': 'O·∫†',
        '√≤e': 'o√®', '√íe': 'O√®', '√íE': 'O√à',
        '√≥e': 'o√©', '√ìe': 'O√©', '√ìE': 'O√â',
        '·ªèe': 'o·∫ª', '·ªée': 'O·∫ª', '·ªéE': 'O·∫∫',
        '√µe': 'o·∫Ω', '√ïe': 'O·∫Ω', '√ïE': 'O·∫º',
        '·ªçe': 'o·∫π', '·ªåe': 'O·∫π', '·ªåE': 'O·∫∏',
        '√πy': 'u·ª≥', '√ôy': 'U·ª≥', '√ôY': 'U·ª≤',
        '√∫y': 'u√Ω', '√öy': 'U√Ω', '√öY': 'U√ù',
        '·ªßy': 'u·ª∑', '·ª¶y': 'U·ª∑', '·ª¶Y': 'U·ª∂',
        '≈©y': 'u·ªπ', '≈®y': 'U·ªπ', '≈®Y': 'U·ª∏',
        '·ª•y': 'u·ªµ', '·ª§y': 'U·ªµ', '·ª§Y': 'U·ª¥',
    }


    @staticmethod
    def normalize_unicode(text):
        char1252 = r'aÃÄ|aÃÅ|aÃâ|aÃÉ|aÃ£|√¢ÃÄ|√¢ÃÅ|√¢Ãâ|√¢ÃÉ|√¢Ã£|ƒÉÃÄ|ƒÉÃÅ|ƒÉÃâ|ƒÉÃÉ|ƒÉÃ£|eÃÄ|eÃÅ|eÃâ|eÃÉ|eÃ£|√™ÃÄ|√™ÃÅ|√™Ãâ|√™ÃÉ|√™Ã£|iÃÄ|iÃÅ|iÃâ|iÃÉ|iÃ£|oÃÄ|oÃÅ|oÃâ|oÃÉ|oÃ£|√¥ÃÄ|√¥ÃÅ|√¥Ãâ|√¥ÃÉ|√¥Ã£|∆°ÃÄ|∆°ÃÅ|∆°Ãâ|∆°ÃÉ|∆°Ã£|uÃÄ|uÃÅ|uÃâ|uÃÉ|uÃ£|∆∞ÃÄ|∆∞ÃÅ|∆∞Ãâ|∆∞ÃÉ|∆∞Ã£|yÃÄ|yÃÅ|yÃâ|yÃÉ|yÃ£|AÃÄ|AÃÅ|AÃâ|AÃÉ|AÃ£|√ÇÃÄ|√ÇÃÅ|√ÇÃâ|√ÇÃÉ|√ÇÃ£|ƒÇÃÄ|ƒÇÃÅ|ƒÇÃâ|ƒÇÃÉ|ƒÇÃ£|EÃÄ|EÃÅ|EÃâ|EÃÉ|EÃ£|√äÃÄ|√äÃÅ|√äÃâ|√äÃÉ|√äÃ£|IÃÄ|IÃÅ|IÃâ|IÃÉ|IÃ£|OÃÄ|OÃÅ|OÃâ|OÃÉ|OÃ£|√îÃÄ|√îÃÅ|√îÃâ|√îÃÉ|√îÃ£|∆†ÃÄ|∆†ÃÅ|∆†Ãâ|∆†ÃÉ|∆†Ã£|UÃÄ|UÃÅ|UÃâ|UÃÉ|UÃ£|∆ØÃÄ|∆ØÃÅ|∆ØÃâ|∆ØÃÉ|∆ØÃ£|YÃÄ|YÃÅ|YÃâ|YÃÉ|YÃ£'
        charutf8 = r'√†|√°|·∫£|√£|·∫°|·∫ß|·∫•|·∫©|·∫´|·∫≠|·∫±|·∫Ø|·∫≥|·∫µ|·∫∑|√®|√©|·∫ª|·∫Ω|·∫π|·ªÅ|·∫ø|·ªÉ|·ªÖ|·ªá|√¨|√≠|·ªâ|ƒ©|·ªã|√≤|√≥|·ªè|√µ|·ªç|·ªì|·ªë|·ªï|·ªó|·ªô|·ªù|·ªõ|·ªü|·ª°|·ª£|√π|√∫|·ªß|≈©|·ª•|·ª´|·ª©|·ª≠|·ªØ|·ª±|·ª≥|√Ω|·ª∑|·ªπ|·ªµ|√Ä|√Å|·∫¢|√É|·∫†|·∫¶|·∫§|·∫®|·∫™|·∫¨|·∫∞|·∫Æ|·∫≤|·∫¥|·∫∂|√à|√â|·∫∫|·∫º|·∫∏|·ªÄ|·∫æ|·ªÇ|·ªÑ|·ªÜ|√å|√ç|·ªà|ƒ®|·ªä|√í|√ì|·ªé|√ï|·ªå|·ªí|·ªê|·ªî|·ªñ|·ªò|·ªú|·ªö|·ªû|·ª†|·ª¢|√ô|√ö|·ª¶|≈®|·ª§|·ª™|·ª®|·ª¨|·ªÆ|·ª∞|·ª≤|√ù|·ª∂|·ª∏|·ª¥'
        char_map = dict(zip(char1252.split('|'), charutf8.split('|')))
        return re.sub(char1252, lambda x: char_map[x.group()], text.strip())
    
    
    @staticmethod
    def normalize_sentence_typing(text, vinai_normalization=False):
        # https://github.com/VinAIResearch/BARTpho/blob/main/VietnameseToneNormalization.md
        if vinai_normalization: # Just simply replace the wrong tone with the correct one defined by VinAI
            for wrong, correct in VietnameseToneNormalizer.VINAI_NORMALIZED_TONE.items():
                text = text.replace(wrong, correct)
            return text.strip()
        
        # Or you can use this algorithm developed by Behitek to normalize Vietnamese typing in a sentence 
        words = text.strip().split()
        for index, word in enumerate(words):
            cw = re.sub(r'(^\p{P}*)([p{L}.]*\p{L}+)(\p{P}*$)', r'\1/\2/\3', word).split('/')
            if len(cw) == 3: cw[1] = VietnameseToneNormalizer.normalize_word_typing(cw[1])
            words[index] = ''.join(cw)
        return ' '.join(words)
    
     
    @staticmethod
    def normalize_word_typing(word):
        if not VietnameseToneNormalizer.is_valid_vietnamese_word(word): return word
        chars, vowel_indexes = list(word), []
        qu_or_gi, tonal_mark = False, 0
        
        for index, char in enumerate(chars):
            if char not in VietnameseToneNormalizer.VOWELS_TO_IDS: continue
            row, col = VietnameseToneNormalizer.VOWELS_TO_IDS[char]
            if index > 0 and (row, chars[index - 1]) in [(9, 'q'), (5, 'g')]:
                chars[index] = VietnameseToneNormalizer.VOWELS_TABLE[row][0]
                qu_or_gi = True
                
            if not qu_or_gi or index != 1: vowel_indexes.append(index)
            if col != 0:
                tonal_mark = col
                chars[index] = VietnameseToneNormalizer.VOWELS_TABLE[row][0]
                
        if len(vowel_indexes) < 2:
            if qu_or_gi:
                index = 1 if len(chars) == 2 else 2
                if chars[index] in VietnameseToneNormalizer.VOWELS_TO_IDS:
                    row, _ = VietnameseToneNormalizer.VOWELS_TO_IDS[chars[index]]
                    chars[index] = VietnameseToneNormalizer.VOWELS_TABLE[row][tonal_mark]
                else: chars[1] = VietnameseToneNormalizer.VOWELS_TABLE[5 if chars[1] == 'i' else 9][tonal_mark]
                return ''.join(chars)
            return word
        
        for index in vowel_indexes:
            row, _ = VietnameseToneNormalizer.VOWELS_TO_IDS[chars[index]]
            if row in [4, 8]: # √™, ∆°
                chars[index] = VietnameseToneNormalizer.VOWELS_TABLE[row][tonal_mark]
                return ''.join(chars)
            
        index = vowel_indexes[0 if len(vowel_indexes) == 2 and vowel_indexes[-1] == len(chars) - 1 else 1] 
        row, _ = VietnameseToneNormalizer.VOWELS_TO_IDS[chars[index]]
        chars[index] = VietnameseToneNormalizer.VOWELS_TABLE[row][tonal_mark]
        return ''.join(chars)
    
    
    @staticmethod
    def is_valid_vietnamese_word(word):
        vowel_indexes = -1 
        for index, char in enumerate(word):
            if char not in VietnameseToneNormalizer.VOWELS_TO_IDS: continue
            if vowel_indexes in [-1, index - 1]: vowel_indexes = index
            else: return False
        return True
    

class VietnameseTextPreprocessor:
    def __init__(self, vncorenlp_dir='./VnCoreNLP', extra_teencodes=None, max_correction_length=512):
        self.vncorenlp_dir = vncorenlp_dir
        self.extra_teencodes = extra_teencodes
        self._load_vncorenlp()
        self._build_teencodes()
        
        self.max_correction_length = max_correction_length
        self.corrector = pipeline(
            'text2text-generation', model='bmd1905/vietnamese-correction-v2', 
            torch_dtype='bfloat16', device_map='auto', num_workers=os.cpu_count()
        )
        print('bmd1905/vietnamese-correction-v2 is loaded successfully.')
        
    
    def _load_vncorenlp(self):
        self.word_segmenter = None
        if self._get_vncorenlp_files('/VnCoreNLP-1.2.jar') and \
           self._get_vncorenlp_files('/models/wordsegmenter/vi-vocab') and \
           self._get_vncorenlp_files('/models/wordsegmenter/wordsegmenter.rdr'):
            self.word_segmenter = VnCoreNLP(self.vncorenlp_dir + '/VnCoreNLP-1.2.jar', annotators='wseg', quiet=False)
            print('VnCoreNLP word segmenter is loaded successfully.')
        else: print('Failed to load VnCoreNLP word segmenter.')
            

    def _get_vncorenlp_files(self, url_slash):
        local_path = self.vncorenlp_dir + url_slash
        if os.path.exists(local_path): return True
        
        # Check if the folder contains the local_path exists, if not, create it.
        if not os.path.exists(os.path.dirname(local_path)):
            os.makedirs(os.path.dirname(local_path))
        
        download_url = 'https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master' + url_slash
        try: 
            print(f'Downloading {download_url} to {local_path}')
            return urllib.request.urlretrieve(download_url, local_path)
        except urllib.error.HTTPError as e:
            print(f'Failed to download {download_url} due to {e}')
            return False
                
        
    def _build_teencodes(self):
        self.teencodes = {
            'ok': ['okie', 'okey', '√¥k√™', 'oki', 'oke', 'okay', 'ok√™'], 
            'kh√¥ng': ['kg', 'not', 'k', 'kh', 'k√¥', 'hok', 'ko', 'khong'], 'kh√¥ng ph·∫£i': ['kp'], 
            'c·∫£m ∆°n': ['tks', 'thks', 'thanks', 'ths', 'thank'], 'h·ªìi ƒë√≥': ['h√πi ƒë√≥'], 'mu·ªën': ['m√∫n'],
            
            'r·∫•t t·ªët': ['perfect', '‚ù§Ô∏è', 'üòç'], 'd·ªÖ th∆∞∆°ng': ['cute'], 'y√™u': ['iu'], 'th√≠ch': ['thik'], 
            't·ªët': [
                'gud', 'good', 'g√∫t', 'tot', 'nice',
                'hehe', 'hihi', 'haha', 'hjhj', 'thick', '^_^', ':)', '=)'
                'üëç', 'üéâ', 'üòÄ', 'üòÇ', 'ü§ó', 'üòô', 'üôÇ'
            ], 
            'b√¨nh th∆∞·ªùng': ['bt', 'bthg'], 'h√†g': ['h√†ng'], 
            'kh√¥ng t·ªët':  ['lol', 'cc', 'huhu', ':(', 'üòî', 'üòì'],
            't·ªá': ['sad', 'por', 'poor', 'bad'], 'gi·∫£ m·∫°o': ['fake'], 
            
            'qu√°': ['wa', 'w√°', 'q√°'], 'ƒë∆∞·ª£c': ['ƒëx', 'dk', 'dc', 'ƒëk', 'ƒëc'], 
            'v·ªõi': ['vs'], 'g√¨': ['j'], 'r·ªìi': ['r'], 'm√¨nh': ['m', 'mik'], 
            'th·ªùi gian': ['time'], 'gi·ªù': ['h'], 
        }
        if self.extra_teencodes: 
            for key, values in self.extra_teencodes.items():
                if any(len(value.split()) > 1 for value in values):
                    raise ValueError('The values for each key in extra_teencodes must be single words.')
                self.teencodes.setdefault(key, []).extend(values)
                
        self.teencodes = {word: key for key, values in self.teencodes.items() for word in values}
        teencode_url = 'https://gist.githubusercontent.com/behitek/7d9441c10b3c2739499fc5a4d9ea06fb/raw/df939245b3e841b62af115be4dcb3516dadc9fc5/teencode.txt'
        response = requests.get(teencode_url)
        
        if response.status_code == 200:
            text_data = StringIO(response.text)
            for pair in text_data:
                teencode, true_text = pair.split('\t')
                self.teencodes[teencode.strip()] = true_text.strip()
            self.teencodes = {k: self.teencodes[k] for k in sorted(self.teencodes)}
        else: print('Failed to fetch teencode.txt from', teencode_url)

    
    def normalize_teencodes(self, text):
        words = []
        for word in text.split():
            words.append(self.teencodes.get(word, word))
        return ' '.join(words)
    
    
    def correct_vietnamese_errors(self, texts):
        # https://huggingface.co/bmd1905/vietnamese-correction-v2
        predictions = self.corrector(texts, max_length=self.max_correction_length, truncation=True)
        return [prediction['generated_text'] for prediction in predictions]
        
    
    def word_segment(self, text):
        if self.word_segmenter: 
            words = self.word_segmenter.tokenize(text)
            return ' '.join(sum(words, [])) # Flatten the list of words
        print('There is no VnCoreNLP word segmenter loaded. Please check the VnCoreNLP jar file.')
        return text
        
    
    def process_text(self, text, normalize_tone=True, segment=True):
        text = text.lower()
        if normalize_tone:
            text = VietnameseToneNormalizer.normalize_unicode(text)
            text = VietnameseToneNormalizer.normalize_sentence_typing(text)
        text = VietnameseTextCleaner.process_text(text)
        text = self.normalize_teencodes(text)
        return self.word_segment(text) if segment else text
    
    
    def process_batch(self, texts, correct_errors=True):
        if correct_errors:
            texts = [self.process_text(text, normalize_tone=True, segment=False) for text in texts]
            texts = self.correct_vietnamese_errors(texts)
            return [self.process_text(text, normalize_tone=False, segment=True) for text in texts]
        return [self.process_text(text, normalize_tone=True, segment=True) for text in texts]
    
    
    def close_vncorenlp(self):
        if self.word_segmenter: 
            print('Closing VnCoreNLP word segmenter...')
            self.word_segmenter.close()
    
    
if __name__ == '__main__':
    # You should be careful when using single word replacement for teencodes, because it can cause misinterpretation. 
    # For example, 'gi√°': ['price', 'gia'] can replace the word 'gia' in 'gia ƒë√¨nh', making it become 'gi√° ƒë√¨nh'.
    extra_teencodes = { 
        'kh√°ch s·∫°n': ['ks'], 'nh√† h√†ng': ['nhahang'], 'nh√¢n vi√™n': ['nv'],
        'c·ª≠a h√†ng': ['store', 'sop', 'shopE', 'shop'], 
        's·∫£n ph·∫©m': ['sp', 'product'], 'h√†ng': ['h√†g'],
        'giao h√†ng': ['ship', 'delivery', 's√≠p'], 'ƒë·∫∑t h√†ng': ['order'], 
        'chu·∫©n ch√≠nh h√£ng': ['authentic', 'aut', 'auth'], 'h·∫°n s·ª≠ d·ª•ng': ['date', 'hsd'],
        'ƒëi·ªán tho·∫°i': ['dt'],  'facebook': ['fb', 'face'],  
        'nh·∫Øn tin': ['nt', 'ib'], 'tr·∫£ l·ªùi': ['tl', 'trl', 'rep'], 
        'feedback': ['fback', 'fedback'], 's·ª≠ d·ª•ng': ['sd'], 'x√†i': ['s√†i'], 
    }
    
    preprocessor = VietnameseTextPreprocessor(vncorenlp_dir='./VnCoreNLP', extra_teencodes=extra_teencodes, max_correction_length=512)
    sample_texts = [
        'Ga gi∆∞∆°ÃÄng kh√¥ng saÃ£ch, nh√¢n vi√™n qu√™n doÃ£n phoÃÄng m√¥Ã£t ngaÃÄy. Ch·∫•t l·ª±∆°ng "ko" ƒëc th·ªèai m√°i üòî',
        'C√°m ∆°n Chudu24 r·∫•t nhi·ªÅuGia ƒë√¨nh t√¥i c√≥ 1 k·ª≥ ngh·ªâ vui v·∫ª.Resort B√¨nh Minh n·∫±m ·ªü v·ªã tr√≠ r·∫•t ƒë·∫πp, theo ƒë√∫ng ti√™u chu·∫©n, c√≤n v·ªÅ ƒÉn s√°ng th√¨ wa d·ªü, ch·ªâ c√≥ 2,3 m√≥n ƒë·ªÉ ch·ªçn',
        'Gi√° c·∫£ h·ª£p l√≠ƒÇn u·ªëng tho·∫£ th√≠chGi·ªØ xe mi·ªÖn ph√≠Kh√¥ng gian b·ªù k√® tho√°ng m√°t C√≥ ph√≤ng m√°y l·∫°nhM·ªói t·ªôi l√∫c qu√°n ƒë√¥ng th√¨ ƒë·ª£i h∆°i l√¢u',
        'May l·∫ßn tr∆∞·ªõc ƒÉn m√¨ k h√†, h√¥m nay ƒÉn th·ª≠ b√∫n b·∫Øp b√≤. C√≥ ch·∫£ t√¥m vi√™n ƒÉn l·∫° l·∫°. T√¥m th√¨ k nhi·ªÅu, nh∆∞ng v·∫´n c√≥ t√¥m th·∫≠t ·ªü nh√¢n b√™n trong. ',
        'Ng·ªìi ƒÉn C∆°m nh√† *ti·ªÅn th√¢n l√† qu√°n B√£o* Ph·∫ßn v·∫≠y l√† 59k nha. Tr∆∞a t·ª´ 10h-14h, chi·ªÅu t·ª´ 16h-19h. √Ä,c√≥ s·ªØa h·∫°t sen ngon l·∫Ømm. #food #foodpic #foodporn #foodholic #yummy #deliciuous'
    ]
    preprocessed_texts = preprocessor.process_batch(sample_texts, correct_errors=True)
    preprocessor.close_vncorenlp()
    print(preprocessed_texts)